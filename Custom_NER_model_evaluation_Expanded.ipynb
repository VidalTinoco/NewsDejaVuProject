{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dabjMtc_N2T1",
    "outputId": "e9adcdf6-3d4a-4779-820c-e39dd13c9fd6"
   },
   "outputs": [],
   "source": [
    "# Install dependencies (run this in a notebook cell, not needed in code below)\n",
    "!pip install transformers datasets seqeval torch\n",
    "!pip install datasets\n",
    "!pip install seqeval --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "05ed4b50e4dd49de816743966d0ab7eb",
      "b856231e112c400f8b3e4d2aeb59f597",
      "2d7b3531e53c4c55acedfda5c7657446",
      "16c5465ad3cb44d28d26a6c7ee2ad470",
      "c5ac68420a7541c58c3097bd2f160bdf",
      "3178f3a1c9e6422a8dd766304308cf3b",
      "91ec277f70d74fb2a11d5fa713208779",
      "8d7241a6e7a74e0db5c856fec9421d60",
      "8a9a9beadf244cc48f5d5daa0a1f4691",
      "8c84fada83cc45a3bf840b902ba75f87",
      "eb1242c9e3264eb39071464a05f345a4",
      "24dedc43e63345bfaa22ba34ff403db6",
      "da816d85166741218eb5dab69cce128b",
      "d4c29667ca51407981e261d6fff61c68",
      "e7455d6b9ceb4c7bbb94d5133b541a31",
      "4f4d1ff61bb5417a81121f8429bb01ad",
      "7632d54c28bb4f41a3727af5c4becdd8",
      "a3166054ca38441e9de4a2e8fa61f1b0",
      "5b1eca1b75e94007aeb9e69734dac494",
      "613e08b764ec44b98f68ce20148514cd",
      "76360fccba0e4df4b01e8b7b14a2559e",
      "07043fa31452426887f59699869d8bdf"
     ]
    },
    "id": "tUEBNtT6vY3p",
    "outputId": "26593377-1e29-4938-a601-3af19ed52fa3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict, Counter\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n",
    "from seqeval.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_score as sk_precision, recall_score as sk_recall, f1_score as sk_f1\n",
    "\n",
    "# Sets device\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Device set to use {'cuda:0' if device == 0 else 'cpu'}\")\n",
    "\n",
    "# Loads dataset\n",
    "dataset = load_dataset(\"dell-research-harvard/newswire\")\n",
    "full_data = dataset[\"train\"]\n",
    "\n",
    "# Selects 10000 random rows from the dataset\n",
    "# Maintaining seed set by teammate\n",
    "random.seed(42)\n",
    "indices = random.sample(range(len(full_data)), 10000)\n",
    "small_corpus = full_data.select(indices)\n",
    "\n",
    "# Shuffle and split\n",
    "random.seed(42)\n",
    "indices = list(range(len(small_corpus)))\n",
    "random.shuffle(indices)\n",
    "split_idx = int(0.8 * len(indices))\n",
    "train_data = small_corpus.select(indices[:split_idx])\n",
    "test_data = small_corpus.select(indices[split_idx:])\n",
    "\n",
    "label_names = [\n",
    "    \"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\",\n",
    "    \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"\n",
    "]\n",
    "\n",
    "# Model dictionary\n",
    "models_to_compare = {\n",
    "    \"Custom Historical NER\": \"dell-research-harvard/historical_newspaper_ner\",\n",
    "    \"RoBERTa-Large (ConLL03)\": \"Jean-Baptiste/roberta-large-ner-english\",\n",
    "    \"BERT-Large (ConLL03)\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "    \"DistilBERT (ConLL03)\": \"elastic/distilbert-base-uncased-finetuned-conll03-english\"\n",
    "}\n",
    "\n",
    "# Custom palette created to match the colours from within the original paper images\n",
    "custom_palette = [\n",
    "    \"#F4A300\",\n",
    "    \"#4BE3AC\",\n",
    "    \"#9B5DE5\",\n",
    "    \"#033268\",\n",
    "    \"#E777C2\",\n",
    "    \"#6BA3D1\",\n",
    "    \"#CFA15A\",\n",
    "    \"#C0D2C1\",\n",
    "    \"#E2B97C\",\n",
    "    \"#EDE9E0\",\n",
    "    \"#FF7F50\",\n",
    "    \"#1B998B\",\n",
    "    \"#8A5082\",\n",
    "    \"#3A86FF\",\n",
    "    \"#BDBF09\",\n",
    "]\n",
    "\n",
    "## Consolidating code\n",
    "#\n",
    "# Defines prediction function\n",
    "def get_predictions(model_name, test_data):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "    pipe = TokenClassificationPipeline(model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=device)\n",
    "\n",
    "    true_labels, pred_labels = [], []\n",
    "\n",
    "    for row in test_data:\n",
    "        words = row[\"ner_words\"]\n",
    "        gold = row[\"ner_labels\"]\n",
    "        sentence = \" \".join(words)\n",
    "        prediction = pipe(sentence)\n",
    "\n",
    "        # Initialize with o\n",
    "        pred_seq = [\"O\"] * len(words)\n",
    "        for ent in prediction:\n",
    "            entity = ent.get(\"entity_group\", ent.get(\"entity\"))\n",
    "            word = ent[\"word\"]\n",
    "\n",
    "            # Match predicted word to a token\n",
    "            for idx, w in enumerate(words):\n",
    "                if word.lower() in w.lower() and pred_seq[idx] == \"O\":\n",
    "                    pred_seq[idx] = \"B-\" + entity\n",
    "                    break\n",
    "\n",
    "        gold_seq = gold\n",
    "        true_labels.append(gold_seq)\n",
    "        pred_labels.append(pred_seq)\n",
    "\n",
    "    return true_labels, pred_labels\n",
    "\n",
    "# Flatten labels\n",
    "def flatten_labels(true_labels, pred_labels):\n",
    "    return (\n",
    "        [label for seq in true_labels for label in seq],\n",
    "        [label for seq in pred_labels for label in seq]\n",
    "    )\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(name, true_labels, pred_labels):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    flat_true, flat_pred = flatten_labels(true_labels, pred_labels)\n",
    "    print(f\"Precision: {sk_precision(flat_true, flat_pred, average='macro', zero_division=1)}\")\n",
    "    print(f\"Recall: {sk_recall(flat_true, flat_pred, average='macro', zero_division=1)}\")\n",
    "    print(f\"F1 Score: {sk_f1(flat_true, flat_pred, average='macro', zero_division=1)}\")\n",
    "    print(\"\\nDetailed Report:\\n\", classification_report(true_labels, pred_labels, zero_division=1))\n",
    "\n",
    "results = {}\n",
    "detailed_results = {}\n",
    "\n",
    "for name, model_path in models_to_compare.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    true, pred = get_predictions(model_path, test_data)\n",
    "    evaluate_model(name, true, pred)\n",
    "    flat_true, flat_pred = flatten_labels(true, pred)\n",
    "    results[name] = {\n",
    "        \"Precision\": sk_precision(flat_true, flat_pred, average=\"macro\", zero_division=1),\n",
    "        \"Recall\": sk_recall(flat_true, flat_pred, average=\"macro\", zero_division=1),\n",
    "        \"F1 Score\": sk_f1(flat_true, flat_pred, average=\"macro\", zero_division=1)\n",
    "    }\n",
    "    detailed_results[name] = classification_report(true, pred, output_dict=True)\n",
    "\n",
    "results_df = pd.DataFrame(results).T.sort_values(\"F1 Score\", ascending=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "8YFMx7Ene14S",
    "outputId": "35f72fd8-59f3-4cfd-f4d9-de2a76dbb908"
   },
   "outputs": [],
   "source": [
    "# Plots global scores\n",
    "def plot_global_results(df):\n",
    "    x = range(len(df))\n",
    "    width = 0.25\n",
    "    colors = custom_palette[:3]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.bar([i - width for i in x], df[\"Precision\"], width=width, label=\"Precision\", color=colors[0])\n",
    "    ax.bar(x, df[\"Recall\"], width=width, label=\"Recall\", color=colors[1])\n",
    "    ax.bar([i + width for i in x], df[\"F1 Score\"], width=width, label=\"F1\", color=colors[2])\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df.index, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Overall Metrics by Model\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_global_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "09OjsdSme2-8",
    "outputId": "0badda8e-8d8b-46c1-aad0-27490e179b12"
   },
   "outputs": [],
   "source": [
    "# Plot per-label F1\n",
    "def plot_label_scores(detail_results):\n",
    "    labels = sorted({label for r in detail_results.values() for label in r if \"avg\" not in label and label != \"accuracy\"})\n",
    "    data = {model: {label: detail_results[model].get(label, {}).get(\"f1-score\", 0) for label in labels} for model in detail_results}\n",
    "    df = pd.DataFrame(data)\n",
    "    df.plot(kind=\"bar\", figsize=(10, 5), colormap='tab20c')\n",
    "    plt.title(\"F1 Score by Entity Label\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.xlabel(\"Entity Label\")\n",
    "    plt.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_label_scores(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 970
    },
    "id": "DcvkDXoZe6rL",
    "outputId": "183634b7-5244-4d94-d9fe-26e96c25f091"
   },
   "outputs": [],
   "source": [
    "# Topic distribution plot\n",
    "def plot_topic_distribution(df):\n",
    "    topic_counts = df['ca_topic'].value_counts()\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(y=topic_counts.index, x=topic_counts.values, palette=custom_palette[:len(topic_counts)])\n",
    "    plt.title('Distribution of Articles by Topic')\n",
    "    plt.xlabel('Number of Articles')\n",
    "    plt.ylabel('Topic')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_topic_distribution(pd.DataFrame(small_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "l2V_KLRue8xl",
    "outputId": "83f6057e-3758-42f0-be3a-1b3ba5919db3"
   },
   "outputs": [],
   "source": [
    "# Entity distribution by topic\n",
    "def plot_entity_distribution_by_topic(df):\n",
    "    entity_counts_by_topic = defaultdict(Counter)\n",
    "    for _, row in df.iterrows():\n",
    "        entity_counts_by_topic[row['ca_topic']].update(row['ner_labels'])\n",
    "\n",
    "    entity_counts_df = pd.DataFrame(entity_counts_by_topic).T.fillna(0).astype(int)\n",
    "    entity_counts_df.plot(kind='barh', stacked=True, figsize=(12, 8), color=custom_palette[:len(entity_counts_df.columns)])\n",
    "    plt.title('Entity Distribution by Topic')\n",
    "    plt.xlabel('Count of Entities')\n",
    "    plt.ylabel('Topic')\n",
    "    plt.legend(title='Entity Types', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_entity_distribution_by_topic(pd.DataFrame(small_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "id": "NTf9PplNe_IV",
    "outputId": "3ddd39cb-c432-4988-d02b-d3575562b1a2"
   },
   "outputs": [],
   "source": [
    "# Entity type distribution\n",
    "def plot_entity_distribution(true_labels, title=\"Entity Type Distribution\"):\n",
    "    flat_labels = [label for seq in true_labels for label in seq]\n",
    "    label_counts = Counter(flat_labels)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.barplot(x=list(label_counts.keys()), y=list(label_counts.values()), palette=custom_palette[:len(label_counts)])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Entity Type\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_entity_distribution(true, title=\"Entity Distribution: Last Model Evaluated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6usSYKMe_E7",
    "outputId": "69528ec4-0579-4c85-a211-13d420394420"
   },
   "outputs": [],
   "source": [
    "# Topic-wise metrics\n",
    "def topic_metrics_by_model():\n",
    "    topic_metrics = defaultdict(lambda: defaultdict(dict))\n",
    "    for model_name, model_path in models_to_compare.items():\n",
    "        true, pred = get_predictions(model_path, test_data)\n",
    "        topics = [row[\"ca_topic\"] for row in test_data]\n",
    "        for topic in set(topics):\n",
    "            subset = [(t, p) for t, p, tp in zip(true, pred, topics) if tp == topic]\n",
    "            if not subset:\n",
    "                continue\n",
    "            flat_true = [label for seq, _ in subset for label in seq]\n",
    "            flat_pred = [label for _, seq in subset for label in seq]\n",
    "            topic_metrics[model_name][topic] = {\n",
    "                \"Precision\": sk_precision(flat_true, flat_pred, average=\"macro\", zero_division=1),\n",
    "                \"Recall\": sk_recall(flat_true, flat_pred, average=\"macro\", zero_division=1),\n",
    "                \"F1 Score\": sk_f1(flat_true, flat_pred, average=\"macro\", zero_division=1)\n",
    "            }\n",
    "    return topic_metrics\n",
    "\n",
    "# Plotting topic-wise metrics\n",
    "topic_metrics = topic_metrics_by_model()\n",
    "\n",
    "df_topic = pd.concat({\n",
    "    model: pd.DataFrame.from_dict(metrics, orient=\"index\")\n",
    "    for model, metrics in topic_metrics.items()\n",
    "}).reset_index()\n",
    "\n",
    "df_topic.columns = [\"Model\", \"Topic\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
    "plot_df = df_topic.melt(id_vars=[\"Model\", \"Topic\"], var_name=\"Metric\", value_name=\"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "Kq8Gkcw1l0vG",
    "outputId": "33d294b6-a50e-4d59-b7c9-074771971e98"
   },
   "outputs": [],
   "source": [
    "plot_df = df_topic.melt(\n",
    "    id_vars=[\"Model\", \"Topic\"],\n",
    "    var_name=\"Metric\",\n",
    "    value_name=\"Score\"\n",
    ")\n",
    "\n",
    "# Top topics based on high scores\n",
    "top_topics = (\n",
    "    plot_df.groupby(\"Topic\")[\"Score\"]\n",
    "    .max()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    "    .index\n",
    ")\n",
    "# Filter for top topics\n",
    "plot_df = plot_df[plot_df[\"Topic\"].isin(top_topics)]\n",
    "plot_df[\"Topic\"] = pd.Categorical(\n",
    "    plot_df[\"Topic\"],\n",
    "    categories=sorted(top_topics),\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_context(\"talk\", font_scale=1.5)\n",
    "g = sns.catplot(\n",
    "    data=plot_df,\n",
    "    kind=\"bar\",\n",
    "    x=\"Score\",\n",
    "    y=\"Topic\",\n",
    "    hue=\"Model\",\n",
    "    col=\"Metric\",\n",
    "    palette=custom_palette,\n",
    "    height=8,\n",
    "    aspect=1.5,\n",
    "    sharex=False,\n",
    ")\n",
    "for ax in g.axes.flat:\n",
    "    ax.tick_params(axis='x', labelsize=18)\n",
    "    ax.tick_params(axis='y', labelsize=18)\n",
    "    ax.set_title(ax.get_title(), fontsize=24)\n",
    "    ax.xaxis.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
