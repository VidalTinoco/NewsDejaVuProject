{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dabjMtc_N2T1",
    "outputId": "55fb1c22-93b5-4841-c453-bdbd47670050"
   },
   "outputs": [],
   "source": [
    "# Install dependencies (run this in a notebook cell, not needed in code below)\n",
    "!pip install transformers datasets seqeval torch\n",
    "!pip install datasets\n",
    "!pip install seqeval --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e7b25af9898e451986d367b10fcb9308",
      "fab0e67bcdd746b7bd8c257ffa2aa880",
      "e57a5f3b8e914e9599fa53ed11b0b2af",
      "60cb53da500f4a72a14468f232d4904d",
      "c822f3e9d2a24d38ab68474b214a3133",
      "c67342020d8b462e82a42c79aeceac51",
      "6dc555201d77426b9976bdddb855d8b3",
      "c5a8cfff012f4ecb94bf79235888f662",
      "0ed671051d9a4b9cae0a6238937371c8",
      "214f7a0f93bb4e5bbce8aaa78392d4b6",
      "f65abce021414bf9b0cbb139a34d90a8",
      "1565e22b59de434a930bdf5565635d4c",
      "2ae66334739f4a8d9226d23b99ccbf23",
      "e5e588083c8642c1a8f31134cc4725d5",
      "c8fbf0d2078b43189688848cff60b47e",
      "c2523aefb6b84032a2ecf54372466934",
      "7ebcb1375d3344bbbecd145fd644de57",
      "c68b0737374641aeb88949b20311545b",
      "f4ff3c9bb62648ebb0412e880d1e1779",
      "0510516b1111496790701e918e9cfbaa",
      "e28a49356694413086d5c0dcb5b66573",
      "100b02440b434da493ad59813aec27a8"
     ]
    },
    "id": "tUEBNtT6vY3p",
    "outputId": "5c8cab34-0e26-45d8-f883-e06f41d0688b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict, Counter\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n",
    "from seqeval.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_score as sk_precision, recall_score as sk_recall, f1_score as sk_f1\n",
    "\n",
    "# Sets device\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Device set to use {'cuda:0' if device == 0 else 'cpu'}\")\n",
    "\n",
    "# Loads dataset\n",
    "dataset = load_dataset(\"dell-research-harvard/newswire\")\n",
    "full_data = dataset[\"train\"]\n",
    "\n",
    "# Selects 10000 random rows from the dataset\n",
    "# Maintaining seed set by teammate\n",
    "random.seed(42)\n",
    "indices = random.sample(range(len(full_data)), 10000)\n",
    "small_corpus = full_data.select(indices)\n",
    "\n",
    "# Shuffle and split\n",
    "random.seed(42)\n",
    "indices = list(range(len(small_corpus)))\n",
    "random.shuffle(indices)\n",
    "split_idx = int(0.8 * len(indices))\n",
    "train_data = small_corpus.select(indices[:split_idx])\n",
    "test_data = small_corpus.select(indices[split_idx:])\n",
    "\n",
    "label_names = [\n",
    "    \"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\",\n",
    "    \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"\n",
    "]\n",
    "\n",
    "# Model dictionary\n",
    "models_to_compare = {\n",
    "    \"Custom Historical NER\": \"dell-research-harvard/historical_newspaper_ner\",\n",
    "    \"RoBERTa-Large (ConLL03)\": \"Jean-Baptiste/roberta-large-ner-english\",\n",
    "    \"BERT-Large (ConLL03)\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "    \"DistilBERT (ConLL03)\": \"elastic/distilbert-base-uncased-finetuned-conll03-english\"\n",
    "}\n",
    "\n",
    "# Custom palette created to match the colours from within the original paper images\n",
    "custom_palette = [\n",
    "    \"#F4A300\",\n",
    "    \"#4BE3AC\",\n",
    "    \"#9B5DE5\",\n",
    "    \"#FF6B6B\",\n",
    "    \"#033268\",\n",
    "    \"#E777C2\",\n",
    "    \"#6BA3D1\",\n",
    "    \"#CFA15A\",\n",
    "    \"#C0D2C1\",\n",
    "    \"#F1C6C6\",\n",
    "    \"#F5D4A1\",\n",
    "    \"#E2B97C\",\n",
    "    \"#EDE9E0\",\n",
    "]\n",
    "\n",
    "## Consolidating code\n",
    "#\n",
    "# Defines prediction function\n",
    "def get_predictions(model_name, test_data):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "    pipe = TokenClassificationPipeline(model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=device)\n",
    "\n",
    "    true_labels, pred_labels = [], []\n",
    "\n",
    "    for row in test_data:\n",
    "        words = row[\"ner_words\"]\n",
    "        gold = row[\"ner_labels\"]\n",
    "        sentence = \" \".join(words)\n",
    "        prediction = pipe(sentence)\n",
    "\n",
    "        # Initialize with o\n",
    "        pred_seq = [\"O\"] * len(words)\n",
    "        for ent in prediction:\n",
    "            entity = ent.get(\"entity_group\", ent.get(\"entity\"))\n",
    "            word = ent[\"word\"]\n",
    "\n",
    "            # Match predicted word to a token\n",
    "            for idx, w in enumerate(words):\n",
    "                if word.lower() in w.lower() and pred_seq[idx] == \"O\":\n",
    "                    pred_seq[idx] = \"B-\" + entity\n",
    "                    break\n",
    "\n",
    "        gold_seq = gold\n",
    "        true_labels.append(gold_seq)\n",
    "        pred_labels.append(pred_seq)\n",
    "\n",
    "    return true_labels, pred_labels\n",
    "\n",
    "# Flatten labels\n",
    "def flatten_labels(true_labels, pred_labels):\n",
    "    return (\n",
    "        [label for seq in true_labels for label in seq],\n",
    "        [label for seq in pred_labels for label in seq]\n",
    "    )\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(name, true_labels, pred_labels):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    flat_true, flat_pred = flatten_labels(true_labels, pred_labels)\n",
    "    print(f\"Precision: {sk_precision(flat_true, flat_pred, average='macro', zero_division=1)}\")\n",
    "    print(f\"Recall: {sk_recall(flat_true, flat_pred, average='macro', zero_division=1)}\")\n",
    "    print(f\"F1 Score: {sk_f1(flat_true, flat_pred, average='macro', zero_division=1)}\")\n",
    "    print(\"\\nDetailed Report:\\n\", classification_report(true_labels, pred_labels, zero_division=1))\n",
    "\n",
    "results = {}\n",
    "detailed_results = {}\n",
    "\n",
    "for name, model_path in models_to_compare.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    true, pred = get_predictions(model_path, test_data)\n",
    "    evaluate_model(name, true, pred)\n",
    "    flat_true, flat_pred = flatten_labels(true, pred)\n",
    "    results[name] = {\n",
    "        \"Precision\": sk_precision(flat_true, flat_pred, average=\"macro\", zero_division=1),\n",
    "        \"Recall\": sk_recall(flat_true, flat_pred, average=\"macro\", zero_division=1),\n",
    "        \"F1 Score\": sk_f1(flat_true, flat_pred, average=\"macro\", zero_division=1)\n",
    "    }\n",
    "    detailed_results[name] = classification_report(true, pred, output_dict=True)\n",
    "\n",
    "results_df = pd.DataFrame(results).T.sort_values(\"F1 Score\", ascending=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "id": "8YFMx7Ene14S",
    "outputId": "1a81e989-4d2a-4f5b-f106-2ada2b4adb2d"
   },
   "outputs": [],
   "source": [
    "# Plots global scores\n",
    "def plot_global_results(df):\n",
    "    x = range(len(df))\n",
    "    width = 0.25\n",
    "    colors = custom_palette[:3]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.bar([i - width for i in x], df[\"Precision\"], width=width, label=\"Precision\", color=colors[0])\n",
    "    ax.bar(x, df[\"Recall\"], width=width, label=\"Recall\", color=colors[1])\n",
    "    ax.bar([i + width for i in x], df[\"F1 Score\"], width=width, label=\"F1\", color=colors[2])\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df.index, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Overall Metrics by Model\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_global_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "09OjsdSme2-8",
    "outputId": "97fa51b2-bc76-4b0b-8ace-f0b8b32311ec"
   },
   "outputs": [],
   "source": [
    "# Plot per-label F1\n",
    "def plot_label_scores(detail_results):\n",
    "    labels = sorted({label for r in detail_results.values() for label in r if \"avg\" not in label and label != \"accuracy\"})\n",
    "    data = {model: {label: detail_results[model].get(label, {}).get(\"f1-score\", 0) for label in labels} for model in detail_results}\n",
    "    df = pd.DataFrame(data)\n",
    "    df.plot(kind=\"bar\", figsize=(10, 5), colormap='tab20c')\n",
    "    plt.title(\"F1 Score by Entity Label\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.xlabel(\"Entity Label\")\n",
    "    plt.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_label_scores(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 964
    },
    "id": "DcvkDXoZe6rL",
    "outputId": "60979897-7a78-4b81-9c19-8779c2efc4e6"
   },
   "outputs": [],
   "source": [
    "# Topic distribution plot\n",
    "def plot_topic_distribution(df):\n",
    "    topic_counts = df['ca_topic'].value_counts()\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(y=topic_counts.index, x=topic_counts.values, palette=custom_palette[:len(topic_counts)])\n",
    "    plt.title('Distribution of Articles by Topic')\n",
    "    plt.xlabel('Number of Articles')\n",
    "    plt.ylabel('Topic')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_topic_distribution(pd.DataFrame(small_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 801
    },
    "id": "l2V_KLRue8xl",
    "outputId": "9c79f78c-04eb-4ee3-99f8-1c134a9f90a1"
   },
   "outputs": [],
   "source": [
    "# Entity distribution by topic\n",
    "def plot_entity_distribution_by_topic(df):\n",
    "    entity_counts_by_topic = defaultdict(Counter)\n",
    "    for _, row in df.iterrows():\n",
    "        entity_counts_by_topic[row['ca_topic']].update(row['ner_labels'])\n",
    "\n",
    "    entity_counts_df = pd.DataFrame(entity_counts_by_topic).T.fillna(0).astype(int)\n",
    "    entity_counts_df.plot(kind='barh', stacked=True, figsize=(12, 8), color=custom_palette[:len(entity_counts_df.columns)])\n",
    "    plt.title('Entity Distribution by Topic')\n",
    "    plt.xlabel('Count of Entities')\n",
    "    plt.ylabel('Topic')\n",
    "    plt.legend(title='Entity Types', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_entity_distribution_by_topic(pd.DataFrame(small_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "id": "NTf9PplNe_IV",
    "outputId": "ea0e712c-9e51-4bd9-fa74-d24c620ad595"
   },
   "outputs": [],
   "source": [
    "# Entity type distribution\n",
    "def plot_entity_distribution(true_labels, title=\"Entity Type Distribution\"):\n",
    "    flat_labels = [label for seq in true_labels for label in seq]\n",
    "    label_counts = Counter(flat_labels)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.barplot(x=list(label_counts.keys()), y=list(label_counts.values()), palette=custom_palette[:len(label_counts)])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Entity Type\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_entity_distribution(true, title=\"Entity Distribution: Last Model Evaluated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "D6usSYKMe_E7",
    "outputId": "040beca5-1b02-48b7-8cf6-e012598a3cc3"
   },
   "outputs": [],
   "source": [
    "# Topic-wise metrics\n",
    "def topic_metrics_by_model():\n",
    "    topic_metrics = defaultdict(lambda: defaultdict(dict))\n",
    "    for model_name, model_path in models_to_compare.items():\n",
    "        true, pred = get_predictions(model_path, test_data)\n",
    "        topics = [row[\"ca_topic\"] for row in test_data]\n",
    "        for topic in set(topics):\n",
    "            subset = [(t, p) for t, p, tp in zip(true, pred, topics) if tp == topic]\n",
    "            if not subset:\n",
    "                continue\n",
    "            flat_true = [label for seq, _ in subset for label in seq]\n",
    "            flat_pred = [label for _, seq in subset for label in seq]\n",
    "            topic_metrics[model_name][topic] = {\n",
    "                \"Precision\": sk_precision(flat_true, flat_pred, average=\"macro\", zero_division=1),\n",
    "                \"Recall\": sk_recall(flat_true, flat_pred, average=\"macro\", zero_division=1),\n",
    "                \"F1 Score\": sk_f1(flat_true, flat_pred, average=\"macro\", zero_division=1)\n",
    "            }\n",
    "    return topic_metrics\n",
    "\n",
    "# Plotting topic-wise metrics\n",
    "topic_metrics = topic_metrics_by_model()\n",
    "\n",
    "df_topic = pd.concat({\n",
    "    model: pd.DataFrame.from_dict(metrics, orient=\"index\")\n",
    "    for model, metrics in topic_metrics.items()\n",
    "}).reset_index()\n",
    "\n",
    "df_topic.columns = [\"Model\", \"Topic\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
    "plot_df = df_topic.melt(id_vars=[\"Model\", \"Topic\"], var_name=\"Metric\", value_name=\"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "id": "Kq8Gkcw1l0vG",
    "outputId": "fd22251f-7c07-42de-e9c7-2e7ff8808cba"
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_context(\"talk\", font_scale=2.5)\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=plot_df,\n",
    "    kind=\"bar\",\n",
    "    x=\"Score\",\n",
    "    y=\"Topic\",\n",
    "    hue=\"Model\",\n",
    "    col=\"Metric\",\n",
    "    palette=custom_palette,\n",
    "    height=20,\n",
    "    aspect=1.2,\n",
    "    sharex=False\n",
    ")\n",
    "\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set_xlabels(\"Score\")\n",
    "g.set_ylabels(\"Topic\")\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.tick_params(axis='x', labelsize=30)\n",
    "    ax.tick_params(axis='y', labelsize=30)\n",
    "    ax.set_title(ax.get_title(), fontsize=50)\n",
    "\n",
    "g._legend.get_title().set_fontsize(15)\n",
    "for label in g._legend.get_texts(15):\n",
    "    label.set_fontsize(15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
